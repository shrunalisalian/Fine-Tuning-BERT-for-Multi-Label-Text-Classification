### ğŸš€ **Fine-Tuning BERT for Multi-Label Text Classification**

**Skills:** NLP, Transformer Models, BERT, PyTorch, Hugging Face, Multi-Label Classification  

This project explores **fine-tuning BERT**, a transformer-based **state-of-the-art NLP model**, for **multi-label text classification**. Unlike traditional classification tasks where each text belongs to a single category, multi-label classification requires assigning **multiple independent labels** to a single input.  

To demonstrate this, we use the **Jigsaw Toxic Comment dataset**, which labels online comments into six toxicity categories. The goal is to build a robust **content moderation model** that can detect **toxic, offensive, and harmful content**â€”a crucial application in **social media monitoring, AI-powered moderation, and online safety**.  

---

## ğŸ¯ **Key Objectives**
âœ… **Fine-Tune BERT for Multi-Label Classification**  
âœ… **Use Sigmoid Activation & Binary Cross-Entropy Loss** for independent label predictions  
âœ… **Leverage GPU Acceleration** with PyTorch for faster training  
âœ… **Evaluate Model Performance** using Precision, Recall, F1-score, and AUC-ROC  
âœ… **Deploy Model for Real-World Text Moderation Applications**  

---

## ğŸ“Š **Dataset Overview: Jigsaw Toxic Comment Dataset**
This dataset consists of **text-based comments** labeled into **six toxicity categories**:  
- **toxic**  
- **severe_toxic**  
- **obscene**  
- **threat**  
- **insult**  
- **identity_hate**  

Unlike **multi-class classification**, where one comment belongs to only one category, **multi-label classification** allows comments to be flagged under **multiple categories simultaneously**.  

---

## ğŸ— **Project Pipeline**
1ï¸âƒ£ **Data Preprocessing** â€“ Cleaning & preparing text for tokenization  
2ï¸âƒ£ **Tokenization with BERT** â€“ Converting text into transformer-compatible input  
3ï¸âƒ£ **Dataset Preparation** â€“ Structuring data into PyTorch `Dataset` & `DataLoader`  
4ï¸âƒ£ **Fine-Tuning BERT** â€“ Training a transformer model on multi-label text data  
5ï¸âƒ£ **Model Evaluation** â€“ Assessing F1-score, accuracy, precision-recall curves  
6ï¸âƒ£ **Deployment Considerations** â€“ Saving & optimizing model for real-world usage  

---

## ğŸ” **Why Multi-Label Classification is Unique?**
âœ… Uses **Sigmoid Activation** (not Softmax) â€“ Each label gets an independent probability  
âœ… Requires **Binary Cross-Entropy (BCE) Loss** â€“ Evaluates each label separately  
âœ… Uses **Multi-Label Metrics** like **Precision-Recall & F1-Score** instead of Accuracy  

### âœ… **Example Query: Multi-Label Prediction**
```python
import torch
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

text = "This comment is awful and extremely offensive!"
tokens = tokenizer(text, return_tensors="pt")

# Forward pass through model
model.eval()
with torch.no_grad():
    predictions = model(**tokens).logits

# Convert logits to probabilities
probabilities = torch.sigmoid(predictions)
labels = (probabilities > 0.5).int()
print(labels)
```
ğŸ’¡ **Output Interpretation:** The model predicts whether the comment is toxic, obscene, or offensive based on thresholding.

---

## ğŸ”¥ **BERT Fine-Tuning for Multi-Label Classification**
**Why BERT?**  
ğŸ”¹ **Context-Aware Representations** â€“ Better understanding of nuanced toxicity  
ğŸ”¹ **Pre-Trained on Large Corpora** â€“ Requires less labeled data to fine-tune  
ğŸ”¹ **Handles Multi-Label Tasks Well** â€“ Supports independent label predictions  

âœ… **Training Loop Example (PyTorch)**  
```python
from torch.optim import AdamW

optimizer = AdamW(model.parameters(), lr=2e-5)

for epoch in range(epochs):
    model.train()
    for batch in train_dataloader:
        inputs, labels = batch
        outputs = model(**inputs).logits
        loss = loss_fn(outputs, labels)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```
ğŸ’¡ **Key Optimizations:**  
âœ” **AdamW Optimizer** â€“ Improves weight updates  
âœ” **Learning Rate Scheduling** â€“ Prevents overfitting  

---

## ğŸ“Š **Model Evaluation & Metrics**
Since **accuracy is misleading** in multi-label tasks, we use:  
âœ” **F1-Score** â€“ Balances precision & recall  
âœ” **AUC-ROC** â€“ Measures overall model effectiveness  
âœ” **Precision-Recall Curves** â€“ Evaluates how well model distinguishes labels  

âœ… **Example: Compute F1-Score**
```python
from sklearn.metrics import f1_score

preds = model.predict(test_dataloader)
f1 = f1_score(y_true, y_pred, average="macro")
print("F1-Score:", f1)
```
ğŸ’¡ **Why F1-Score?** It ensures **both precision (low false positives) & recall (low false negatives)** are optimized.

---

## ğŸ”® **Future Scope & Enhancements**
ğŸ”¹ **Fine-Tune DistilBERT for Faster Inference**  
ğŸ”¹ **Use Ensemble Models for Robustness**  
ğŸ”¹ **Integrate Explainability (SHAP/LIME) for Transparency**  
ğŸ”¹ **Deploy Model as an API for Real-Time Toxicity Detection**  

---

## ğŸ“Œ **References & Tutorials Used**
This project was built using the following tutorials & resources:  
ğŸ”¹ [Hugging Face BERT Fine-Tuning Guide](https://huggingface.co/transformers/)  
ğŸ”¹ [Jigsaw Toxic Comment Classification Kaggle Competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/)  
ğŸ”¹ [PyTorch Transformers Documentation](https://pytorch.org/)  

---

## ğŸ›  **How to Run This Project**
1ï¸âƒ£ Clone the repo:  
   ```bash
   git clone https://github.com/shrunalisalian/bert-multi-label-text-classification.git
   ```
2ï¸âƒ£ Install dependencies:  
   ```bash
   pip install -r requirements.txt
   ```
3ï¸âƒ£ Run the Jupyter Notebook:  
   ```bash
   jupyter notebook fine-tuning-bert-multi-label.ipynb
   ```

---

## ğŸ“Œ **Connect with Me**
- **LinkedIn:** [Shrunali Salian](https://www.linkedin.com/in/shrunali-salian/)  
- **Portfolio:** [https://portfolio-shrunali-suresh-salians-projects.vercel.app/](#)  
- **Email:** [Your Email](#)  
